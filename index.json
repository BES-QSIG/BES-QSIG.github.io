[{"authors":["Laura Graham"],"categories":[],"content":"We have migrated from our old Wordpress site in order to keep everything in one place. Our Wordpress site will function as an archive, but new material will be updated here.\nOur aim as a group is to:\n Support quantitative skills development for all ecologists Improve dissemination of new quantitative methods to their users Provide a forum to advance quantitative ecology in its own right  If you have ideas for events, workshops or courses that you would like to see us organise (or would like to organise yourself with our support), please contact us.\n","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550966400,"objectID":"ffd1fbf6b98f3684b639162e208d983e","permalink":"https://BES-QSIG.github.io/about/qesig/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/about/qesig/","section":"about","summary":"We have migrated from our old Wordpress site in order to keep everything in one place. Our Wordpress site will function as an archive, but new material will be updated here.\nOur aim as a group is to:\n Support quantitative skills development for all ecologists Improve dissemination of new quantitative methods to their users Provide a forum to advance quantitative ecology in its own right  If you have ideas for events, workshops or courses that you would like to see us organise (or would like to organise yourself with our support), please contact us.","tags":[],"title":"Quantitative Ecology Special Interest Group QE-SIG","type":"about"},{"authors":["Laura Graham"],"categories":[],"content":"We have migrated from our old Wordpress site in order to keep everything in one place. Our Wordpress site will function as an archive, but new material will be updated here.\nOur aim as a group is to:\n Support quantitative skills development for all ecologists Improve dissemination of new quantitative methods to their users Provide a forum to advance quantitative ecology in its own right  If you have ideas for events, workshops or courses that you would like to see us organise (or would like to organise yourself with our support), please contact us.\n","date":1543276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543276800,"objectID":"6c2e4b1cbfc388c512d2100ea72573f4","permalink":"https://BES-QSIG.github.io/post/welcome-to-new-site/","publishdate":"2018-11-27T00:00:00Z","relpermalink":"/post/welcome-to-new-site/","section":"post","summary":"We have migrated from our old Wordpress site in order to keep everything in one place. Our Wordpress site will function as an archive, but new material will be updated here.\nOur aim as a group is to:\n Support quantitative skills development for all ecologists Improve dissemination of new quantitative methods to their users Provide a forum to advance quantitative ecology in its own right  If you have ideas for events, workshops or courses that you would like to see us organise (or would like to organise yourself with our support), please contact us.","tags":[],"title":"Welcome to the new Quantitative Ecology Site","type":"post"},{"authors":null,"categories":null,"content":" Pre-course Instructions Below see instructions on the software and R packages to install before you arrive. Note that you will receive the greatest benefit of the course if you have already installed the software and packages.\nYou will need to have an up-to-date installation of R and RStudio.\nYou will need to install PostgreSQL.\nYou will also need to install some R packages using the below code:\ninstall.packages(\u0026quot;rmarkdown\u0026quot;) install.packages(\u0026quot;tidyverse\u0026quot;) install.packages(\u0026quot;broom\u0026quot;) install.packages(\u0026quot;ggExtra\u0026quot;) install.packages(\u0026quot;maps\u0026quot;) install.packages(\u0026quot;RColorBrewer\u0026quot;) install.packages(\u0026quot;gridExtra\u0026quot;) install.packages(\u0026quot;rgbif\u0026quot;) install.packages(\u0026quot;CoordinateCleaner\u0026quot;) install.packages(\u0026quot;ggrepel\u0026quot;) install.packages(\u0026quot;png\u0026quot;) install.packages(\u0026quot;mapdata\u0026quot;) install.packages(\u0026quot;ggthemes\u0026quot;) install.packages(\u0026quot;sf\u0026quot;) install.packages(\u0026quot;RPostgreSQL\u0026quot;)  Getting here The course is taking place in the Waterhouse Room at the Foresight Centre, University of Liverpool.\n Road From the M62 (end of motorway 3 miles)\nFrom the M53( Wirral/Mersey Tunnels 3\u0026frasl;4 mile)\nPlease use L3 5DA for sat nav\nRail Liverpool Lime St Station is a 10 minute walk.\nFor local public transport please contact Merseytravel\nAir Liverpool John Lennon Airport is 7 Miles away from the Foresight Centre.\nThe Airlink 500 loops to the city centre every 30 minutes.\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"93be671fe3982b19ca72f025836fc0c9","permalink":"https://BES-QSIG.github.io/courses/advancingr/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/courses/advancingr/","section":"courses","summary":"Pre-course Instructions Below see instructions on the software and R packages to install before you arrive. Note that you will receive the greatest benefit of the course if you have already installed the software and packages.\nYou will need to have an up-to-date installation of R and RStudio.\nYou will need to install PostgreSQL.\nYou will also need to install some R packages using the below code:\ninstall.packages(\u0026quot;rmarkdown\u0026quot;) install.packages(\u0026quot;tidyverse\u0026quot;) install.","tags":null,"title":"Advancing Your R","type":"docs"},{"authors":null,"categories":null,"content":" PART I: Create a reproducible report using Markdown 1. What is R Markdown? R Markdown allows you to create documents that serve as a neat record of your analysis. In the world of reproducible research, we want other researchers to easily understand what we did in our analysis. You might choose to create an R markdown document as an appendix to a paper or project assignment that you are doing, upload it to an online repository such as Github, or simply to keep as a personal record so you can quickly look back at your code and see what you did. R Markdown presents your code alongside its output (graphs, tables, etc.) with conventional text to explain it, a bit like a notebook. Your report can also be what you base your future methods and results sections in your manuscripts, thesis chapters, etc.\nR Markdown uses markdown syntax. Markdown is a very simple \u0026lsquo;markup\u0026rsquo; language which provides methods for creating documents with headers, images, links etc. from plain text files, while keeping the original plain text file easy to read. You can convert Markdown documents to other file types like .html or .pdf.\n \n2. Download R Markdown To get R Markdown working in RStudio, the first thing you need is the rmarkdown package, which you can get from CRAN by running the following commands in R or RStudio:\ninstall.packages(\u0026quot;rmarkdown\u0026quot;) library(rmarkdown)  3. The different parts of an R Markdown file The YAML Header At the top of any R Markdown script is a YAML header section enclosed by ---. By default this includes a title, author, date and the file type you want to output to. Many other options are available for different functions and formatting, see here for .html options and here for .pdf options. Rules in the header section will alter the whole document.\nAdd your own details at the top of your.Rmd script, e.g.:\n--- title: \u0026quot;The tidyverse in action - population change in forests\u0026quot; author: Gergana Daskalova date: 22/Oct/2016 output: html_document ---  By default, the title, author, date and output format are printed at the top of your .html document.\nNow that we have our first piece of content, we can test the .Rmd file by compiling it to .html. To compile your .Rmd file into a .html document, you should press the Knit button in the taskbar:\nNot only does a preview appear in the Viewer window in RStudio, but it also saves a .html file to the same folder where you saved your .Rmd file.\n\nCode Chunks Have a read through the text below to learn a bit more about how Markdown works and then you can start compiling the rest of your .Md file.\nThe setup chunk This code chunk appears in .Md files in R by default, it won\u0026rsquo;t appear in your html or pdf document, it just sets up the document.\n```{r setup, include = FALSE} knitr::opts_chunk$set(echo = TRUE) ```  The rest of the code chunks This is where you can add your own code, accompanying explanation and any outputs. Code that is included in your .Rmd document should be enclosed by three backwards apostrophes ``` (grave accents!). These are known as code chunks and look like this (no need to copy this, just an example):\n```{r} norm \u0026lt;- rnorm(100, mean = 0, sd = 1) ```  Inside the curly brackets is a space where you can assign rules for that code chunk. The code chunk above says that the code is R code.\nIt\u0026rsquo;s important to remember when you are creating an R Markdown file that if you want to run code that refers to an object, for example:\n```{r} plot(dataframe) ```  You have to include the code that defines what dataframe is, just like in a normal R script. For example:\n```{r} A \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;) B \u0026lt;- c(5, 10, 15, 20) dataframe \u0026lt;- data.frame(A, B) plot(dataframe) ```  Or if you are loading a dataframe from a .csv file, you must include the code in the .Rmd:\n```{r} dataframe \u0026lt;- read.csv(\u0026quot;~/Desktop/Code/dataframe.csv\u0026quot;) ```  Similarly, if you are using any packages in your analysis, you have to load them in the .Rmd file using library() like in a normal R script.\n```{r} library(dplyr) ```  Hiding code chunks If you don\u0026rsquo;t want the code of a particular code chunk to appear in the final document, but still want to show the output (e.g. a plot), then you can include echo = FALSE in the code chunk instructions.\n```{r, echo = FALSE} A \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;) B \u0026lt;- c(5, 10, 15, 20) dataframe \u0026lt;- data.frame(A, B) plot(dataframe) ```  Sometimes, you might want to create an object, but not include both the code and its output in the final .html file. To do this you can use, include = FALSE. Be aware though, when making reproducible research it\u0026rsquo;s often not a good idea to completely hide some part of your analysis:\nREMEMBER: R Markdown doesn\u0026rsquo;t pay attention to anything you have loaded in other R scripts, you have to load all objects and packages in the R Markdown script.\nNow you can start copying across the code from your tidyverse script and insert it into a code chunk in your .Rmd document. Better not to do it all at once, you can start with the first parts of the tidyverse script and gradually add on more after you\u0026rsquo;ve seen what the .Rmd output looks like.\nYou can run an individual chunk of code at any time by placing your cursor inside the code chunk and selecting Run -\u0026gt; Run Current Chunk:\nSummary of code chunk instructions .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;} .tg .tg-yw4l{vertical-align:top}   Rule Example\n(default) Function   eval eval=TRUE Is the code run and the results included in the output?   include include=TRUE Are the code and the results included in the output?   echo echo=TRUE Is the code displayed alongside the results?   warning warning=TRUE Are warning messages displayed?   error error=FALSE Are error messages displayed?   message message=TRUE Are messages displayed?   tidy tidy=FALSE Is the code reformatted to make it look “tidy”?   results results=\u0026ldquo;markup\u0026rdquo;  How are results treated?  \u0026ldquo;hide\u0026rdquo; = no results \u0026ldquo;asis\u0026rdquo; = results without formatting \u0026ldquo;hold\u0026rdquo; = results only compiled at end of chunk (use if many commands act on one object)   cache cache=FALSE Are the results cached for future renders?   comment comment=\u0026ldquo;##\u0026rdquo; What character are comments prefaced with?   fig.width, fig.height fig.width=7 What width/height (in inches) are the plots?   fig.align fig.align=\u0026ldquo;left\u0026rdquo; \u0026ldquo;left\u0026rdquo; \u0026ldquo;right\u0026rdquo; \u0026ldquo;center\u0026rdquo;  \n4. Inserting Figures By default, RMarkdown will place graphs by maximising their height, while keeping them within the margins of the page and maintaining aspect ratio. If you have a particularly tall figure, this can mean a really huge graph. To manually set the figure dimensions, you can insert an instruction into the curly braces:\n```{r, fig.width = 2.5, fig.height = 7.5} ggplot(df, aes(x = x, y = y) + geom_point() ```  5. Inserting Tables R Markdown can print the contents of a data frame easily by enclosing the name of the data frame in a code chunk:\n```{r} dataframe ```  This can look a bit messy, especially with data frames with a lot of columns. You can also use a table formatting function, e.g. kable() from the knitr package. The first argument tells kable to make a table out of the object dataframe and that numbers should have two significant figures. Remember to load the knitr package in your .Rmd file, if you are using the kable() function.\n```{r} kable(dataframe, digits = 2) ```  If you want a bit more control over the content of your table you can use pander() from the pander package. Imagine I want the 3rd column to appear in italics:\n```{r} emphasize.italics.cols(3) # Make the 3rd column italics pander(richness_abund) # Create the table ```  Now that you have started your Markdown document, you can use that when completing the next part of the tutorial, i.e., inserting the code that follows into code chunks and then generating a report at the end of this tutorial.\nPART II: Analyse and visualise data using the tidyverse Learning Objectives 1. Intro to the tidyverse How to analyse population change of forest vertebrates  How to write a custom ggplot2 function How to use gather() and spread() from the tidyr package How to parse numbers using parse_number() from the readr package How to use the distinct() function from dplyr How to use the filter() function from dplyr How to use the mutate() function from dplyr How to use the summarise()/summarize() function from dplyr How to use the tidy() function from the broom package to summarise model results How to use the select() function from dplyr  In this tutorial, we will focus on how to efficiently format, manipulate and visualise large datasets. We will use the tidyr and dplyr packages to clean up data frames and calculate new variables. We will use the broom and purr packages to make the modelling of thousands of population trends more efficient. We will use the ggplot2 package to make graphs, maps of occurrence records, and to visualise ppulation trends and then we will arrange all of our graphs together using the gridExtra package.\nWe will be working with population data from the Living Planet Database and red deer occurrence data from the Global Biodiversity Information Facility, both of which are publicly available datasets.\nFirst, we will model population change for vertebrate forest species to see whether greater population change is found for longer duration studies.\nBecause we have created a version-controlled R project using the repository for the workshop, we are already in the right working directory, i.e. the folder that contains all the data and other files, thus there is no need for us to set a working directory at the start of the script, unless we explicitly want to change it for some reason.\nHere are the packages we need. Note that not all tidyverse packages load automatically with library(tidyverse) - only the core ones do, so you need to load broom separately. If you don\u0026rsquo;t have some of the packages installed, you can install them using ìnstall.packages(\u0026quot;package-name\u0026quot;).\n# Packages ---- library(tidyverse) # Hadley Wickham's tidyverse - the theme of this tutorial library(broom) # To summarise model outputs library(ggExtra) # To make pretty graphs - addon package to ggplot2 library(maps) # To make pretty maps - warning: maps masks map from purr! library(RColorBrewer) # To make pretty colours library(gridExtra) # To arrange multi-plot panels  If you\u0026rsquo;ve ever tried to perfect your ggplot2 graphs, you might have noticed that the lines starting with theme() quickly pile up: you adjust the font size of the axes and the labels, the position of the title, the background colour of the plot, you remove the grid lines in the background, etc. And then you have to do the same for the next plot, which really increases the amount of code you use. Here is a simple solution: create a customised theme that combines all the theme() elements you want and apply it to your graphs to make things easier and increase consistency. You can include as many elements in your theme as you want, as long as they don\u0026rsquo;t contradict one another and then when you apply your theme to a graph, only the relevant elements will be considered - e.g. for our graphs we won\u0026rsquo;t need to use legend.position, but it\u0026rsquo;s fine to keep it in the theme in case any future graphs we apply it to do have the need for legends.\n# Setting a custom ggplot2 function --- # *** Functional Programming *** # This function makes a pretty ggplot theme # This function takes no arguments! theme_LPD \u0026lt;- function(){ theme_bw()+ theme(axis.text.x = element_text(size = 12, vjust = 1, hjust = 1), axis.text.y = element_text(size = 12), axis.title.x = element_text(size = 14, face = \u0026quot;plain\u0026quot;), axis.title.y = element_text(size = 14, face = \u0026quot;plain\u0026quot;), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \u0026quot;cm\u0026quot;), plot.title = element_text(size = 20, vjust = 1, hjust = 0.5), legend.text = element_text(size = 12, face = \u0026quot;italic\u0026quot;), legend.title = element_blank(), legend.position = c(0.5, 0.8)) }  Load population trend data The data are in a .RData format, as those are quicker to use, since .Rdata files are more compressed. Of course, a drawback is that .RData files can only be used within R, whereas .csv files are more transferable.\n# Load data ---- load(\u0026quot;LPDdata_Feb2016.RData\u0026quot;) # Inspect data ---- head(LPDdata_Feb2016)   \nAt the moment, each row contains a population that has been monitored over time and towards the right of the data frame there are lots of columns with population estimates for each year. To make this data \u0026ldquo;tidy\u0026rdquo; (one column per variable) we can use gather() to transform the data so there is a new column containing all the years for each population and an adjacent column containing all the population estimates for those years.\nThis takes our original dataset LPIdata_Feb2016 and creates a new column called year, fills it with column names from columns 26:70 and then uses the data from these columns to make another column called pop.\n# Format data for analysis ---- # Transform from wide to long format usign gather (opposite is spread) # *** gather() function from the dplyr package in the tidyverse *** LPD_long \u0026lt;- gather(data = LPDdata_Feb2016, key = \u0026quot;year\u0026quot;, value = \u0026quot;pop\u0026quot;, select = 26:70)  Because column names are coded in as characters, when we turned the column names (1970, 1971, 1972, etc.) into rows, R automatically put an X in front of the numbers to force them to remain characters. We don\u0026rsquo;t want that, so to turn year into a numeric variable, use the parse_number() function from the readr package. We can also make all the column names lowercase and remove some of the funky characters in the country column - strange characters mess up things in general, e.g. when you want to save files, push them to GitHub, etc.\n# Get rid of the X in front of years # *** parse_number() from the readr package in the tidyverse *** LPD_long$year \u0026lt;- parse_number(LPD_long$year) # Rename variable names for consistency names(LPD_long) names(LPD_long) \u0026lt;- tolower(names(LPD_long)) names(LPD_long) # Create new column with genus and species together LPD_long$species.name \u0026lt;- paste(LPD_long$genus, LPD_long$species, sep = \u0026quot; \u0026quot;) # Get rid of strange characters like \u0026quot; / \u0026quot; LPD_long$country.list \u0026lt;- gsub(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;, LPD_long$country.list, fixed = TRUE) LPD_long$biome \u0026lt;- gsub(\u0026quot;/\u0026quot;, \u0026quot;\u0026quot;, LPD_long$biome, fixed = TRUE) # Examine the tidy data frame head(LPD_long)   \nNow that our dataset is tidy we can get it ready for our analysis. We want to only use populations that have more than 5 years of data to make sure our analysis has enough data to capture population change. We should also scale the population data, because since the data come from many species, the units and magnitude of the data are very different - imagine tiny fish whose abundance is in the millions, and large carnivores whose abundance is much smaller. Scaling also normalises the data, as later on we will be using linear models assuming a normal distribution. To do all of this in one go, we can use pipes.\nPipes (%\u0026gt;%) are a way of streamlining data manipulation - imagine all of your data coming in one end of the pipe, while they are in there, they are manipulated, summarised, etc., then the output (e.g. your new data frame or summary statistics) comes out the other end of the pipe. At each step of the pipe processing, the pipe is using the ouput of the previous step.\n# Data manipulation ---- # *** piping from from dplyr LPD_long2 \u0026lt;- LPD_long %\u0026gt;% # Remove duplicate rows # *** distinct() function from dplyr distinct(LPD_long) %\u0026gt;% # remove NAs in the population column # *** filter() function from dplyr filter(is.finite(pop)) %\u0026gt;% # Group rows so that each group is one population # *** group_by() function from dplyr group_by(id) %\u0026gt;% # Make some calculations # *** mutate() function from dplyr mutate(maxyear = max(year), minyear = min(year), # Calculate duration duration = maxyear - minyear, # Scale population trend data scalepop = (pop - min(pop))/(max(pop) - min(pop))) %\u0026gt;% # Keep populations with \u0026gt;5 years worth of data and calculate length of monitoring filter(is.finite(scalepop), length(unique(year)) \u0026gt; 5) %\u0026gt;% # Remove any groupings you've greated in the pipe ungroup()  Now we can explore our data a bit. Let\u0026rsquo;s create a few basic summary statistics for each biome and store them in a new data frame:\n# Calculate summary statistics for each biome LPD_biome_sum \u0026lt;- LPD_long2 %\u0026gt;% # Group by biome group_by(biome) %\u0026gt;% # Create columns, number of populations # *** summarise()/summarize() function from dplyr in the tidyverse *** summarise(populations = n(), # Calculate the mean study length mean_study_length_years = mean(duration), # Model sampling method dominant_sampling_method = names(which.max(table(sampling.method))), # Model unit type dominant_units = names(which.max(table(units)))) %\u0026gt;% # Remove any groupings you've greated in the pipe ungroup() # Take a look at some of the records head(LPD_biome_sum)  Next we will explore how populations from two of these biomes, temperate coniferous and temperate broadleaf forests, have changed over the monitoring duration. We will make the biome variable a factor (before it is just a character variable), so that later on we can make graphs based on the two categories (coniferous and broadleaf forests). We\u0026rsquo;ll use the filter() function from dplyr to subset the data to just the forest species.\n# Subset to just temperate forest species ----- # Notice the difference between | and \u0026amp; when filtering # | is an \u0026quot;or\u0026quot; whereas \u0026amp; is \u0026quot;and\u0026quot;, i.e. both conditions have to be met # at the same time LPD_long2$biome \u0026lt;- as.factor(LPD_long2$biome) LPD.forest \u0026lt;- filter(LPD_long2, biome == \u0026quot;Temperate broadleaf and mixed forests\u0026quot; | biome == \u0026quot;Temperate coniferous forests\u0026quot;)  Before running models, it\u0026rsquo;s a good idea to visualise our data to explore what kind of distribution we are working with.\nThe gg in ggplot2 stands for grammar of graphics. Writing the code for your graph is like constructing a sentence made up of different parts that logically follow from one another. In a data visualisation context, the different elements of the code represent layers - first you make an empty plot, then you add a layer with your data points, then your measure of uncertainty, the axis labels and so on.\n When using ggplot2, you usually start your code with ggplot(your_data, aes(x = independent_variable, y = dependent_variable)), then you add the type of plot you want to make using + geom_boxplot(), + geom_histogram(), etc. aes stands for aesthetics, hinting to the fact that using ggplot2 you can make aesthetically pleasing graphs - there are many ggplot2 functions to help you clearly communicate your results, and we will now go through some of them.\nWhen we want to change the colour, shape or fill of a variable based on another variable, e.g. colour-code by species, we include colour = species inside the aes() function. When we want to set a specific colour, shape or fill, e.g. colour = \u0026quot;black\u0026quot;, we put that outside of the aes() function.\nWe will see our custom theme theme_LPD() in action as well!\n# Data visualisation ---- # Data distribution - a histogram (forest.hist \u0026lt;- ggplot(LPD.forest, aes(x = scalepop)) + geom_histogram(aes(fill = biome), position = \u0026quot;identity\u0026quot;, colour = \u0026quot;grey40\u0026quot;) + geom_vline(aes(xintercept = mean(scalepop)), # Adding a line for mean abundance colour = \u0026quot;darkred\u0026quot;, linetype = \u0026quot;dashed\u0026quot;, size = 1) + scale_fill_manual(values = c(\u0026quot;#66CD00\u0026quot;, \u0026quot;#53868B\u0026quot;)) + theme_LPD() + labs(title = \u0026quot;a) Data distribution\\n\u0026quot;, x = \u0026quot;\\nScaled population size\u0026quot;, y = \u0026quot;Count\\n\u0026quot;) + # \\n adds a blank line guides(fill = F)) # Hiding the legend - this will be a two plot panel # thus we don't need the same legend twice  Next up we can explore for how long populations have been monitored in the two biomes using a density histogram.\n# Density histogram of duration of studies in the two biomes (duration.forests \u0026lt;- ggplot(LPD.forest, aes(duration, colour = biome)) + stat_density(geom = \u0026quot;line\u0026quot;, size = 2, position = \u0026quot;identity\u0026quot;) + theme_LPD() + scale_colour_manual(values = c(\u0026quot;#66CD00\u0026quot;, \u0026quot;#53868B\u0026quot;)) + labs(x = \u0026quot;\\nYears\u0026quot;, y = \u0026quot;Density\\n\u0026quot;, title = \u0026quot;b) Monitoring duration\\n\u0026quot;))  We\u0026rsquo;ll use the grid.arrange function from the gridExtra package to combine the two plots in a panel. You can specify the number of columns using ncol = and the number of rows using nrow =.\n# Arrange in a panel and save forest.panel \u0026lt;- grid.arrange(forest.hist, duration.forests, ncol = 2) ggsave(forest.panel, file = \u0026quot;forest_panel.png\u0026quot;, height = 5, width = 10)   \nWe are now ready to model how each population has changed over time. There are 1785 populations, so with this one code chunk, we will run 1785 models and tidy up their outputs. You can read through the line-by-line comments to get a feel for what each line of code is doing.\nOne specific thing to note is that when you add the lm() function in a pipe, you have to add data = ., which means use the outcome of the previous step in the pipe for the model.\n# Calculate population change for each forest population # 1785 models in one go! # Using a pipe forest.slopes \u0026lt;- LPD.forest %\u0026gt;% # Group by the key variables that we want to interate over group_by(decimal.latitude, decimal.longitude, class, species.name, id, duration, location.of.population) %\u0026gt;% # Create a linear model for each group do(mod = lm(scalepop ~ year, data = .)) %\u0026gt;% # Extract model coefficients using tidy() from the # *** tidy() function from the broom package *** tidy(mod) %\u0026gt;% # Filter out slopes and remove intercept values filter(term == \u0026quot;year\u0026quot;) %\u0026gt;% # Get rid of the column term as we don't need it any more # *** select() function from dplyr in the tidyverse *** dplyr::select(-term) %\u0026gt;% # Remove any groupings you've greated in the pipe ungroup()  We are ungrouping at the end of our pipe just because otherwise the object remains grouped and later on that might cause problems, if we forget about it.\nNow we can visualise the outputs of all our models and see how they vary based on study duration. We will add density histograms along the margins of the graph which makes for a more informative graph using the ggMarginal() function from the ggExtra package. Note that ggExtra is also an addin in RStudio, so for future reference, if you select some ggplot2 code, then click on Addins/ggplot2 Marginal plots (the menu is in the middle top part of the screen), you can customise marginal histograms and the code gets automatically generated.\n# Visualising model outputs ---- # Plotting slope estimates and standard errors for all populations and adding histograms along the margins (all.slopes \u0026lt;- ggplot(forest.slopes, aes(x = duration, y = estimate)) + geom_pointrange(aes(ymin = estimate - std.error, ymax = estimate + std.error), alpha = 0.3, size = 0.3) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;) + theme_LPD() + ylab(\u0026quot;Population change\\n\u0026quot;) + xlab(\u0026quot;\\nDuration (years)\u0026quot;)) (density.slopes \u0026lt;- ggExtra::ggMarginal( p = all.slopes, type = 'density', margins = 'both', size = 5, col = 'gray40', fill = 'gray' )) # Save the plot ggsave(density.slopes, filename = \u0026quot;slopes_duration.png\u0026quot;, height = 6, width = 6)   \n\n2. Using pipes to make figures with large datasets How to print plots of population change for multiple taxa\n How to set up file paths and folders in R How to use a pipe to plot many plots by taxa How to use the purrr package and functional programming  In the next part of the tutorial, we will focus on automating iterative actions, for example when we want to create the same type of graph for different subsets of our data. In our case, we will make histograms of the population change experienced by different vertebrate taxa in forests. When making multiple graphs at once, we have to specify the folder where they will be saved first:\n# PART 2: Using pipes to make figures with large datasets ---- # Make histograms of slope estimates for each taxa ----- # Set up new folder for figures # Set path to relevant path on your computer/in your repository path1 \u0026lt;- \u0026quot;Taxa_Forest_LPD/\u0026quot; # Create new folder dir.create(path1)  There isn\u0026rsquo;t a right answer here, there are different ways to achieve the same result and you can decide which one works best for your workflow. First we will use dplyr and pipes %\u0026gt;%. Since we want one graph per taxa, we are going to group by the class variable. You can add functions that are not part of the dplyr package to pipes using do - in our case, we are saying that we want R to do our requested action (making and saving the histograms) for each taxa.\n# First we will do this using dplyr and a pipe forest.slopes %\u0026gt;% # Select the relevant data dplyr::select(id, class, species.name, estimate) %\u0026gt;% # Group by taxa group_by(class) %\u0026gt;% # Save all plots in new folder do(ggsave(ggplot(., aes(x = estimate)) + # Add histograms geom_histogram(colour = \u0026quot;darkgreen\u0026quot;, fill = \u0026quot;darkgreen\u0026quot;, binwidth = 0.02) + # Use custom theme theme_LPD() + # Add axis lables xlab(\u0026quot;Rate of population change (slopes)\u0026quot;), # Set up file names to print to filename = gsub(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;, paste0(path1, unique(as.character(.$class)), \u0026quot;.pdf\u0026quot;)), device = \u0026quot;pdf\u0026quot;))  A warning message pops up: Error: Results 1, 2, 3, 4 must be data frames, not NULL - you can ignore this, it\u0026rsquo;s because the do() function expects a data frame as an output, but in our case we are making graphs, not data frames.\nIf you go check out your folder now, you should see four histograms, one per taxa:  \nAnother way to make all those histograms in one go is by creating a function for it. In general, whenever you find yourself copying and pasting lots of code only to change the object name, you\u0026rsquo;re probably in a position to swap all the code with a function - you can then apply the function using the purrr package.\nBut what is purrr? It is a way to \u0026ldquo;map\u0026rdquo; or \u0026ldquo;apply\u0026rdquo; functions to data. Note that there are functions from other packages also called map(), which is why we are specifying we want the map() function from the purrr package. Here we will first format the data taxa.slopes and then we will map it to the mean fuction:\nWe have to change the format of the data, in our case we will split the data using spread() from the tidyr package.\n# Here we select the relevant data # Let's get rid of the other levels of 'class' forest.slopes$class \u0026lt;- as.character(forest.slopes$class) # Selecting the relevant data and splitting it into a list taxa.slopes \u0026lt;- forest.slopes %\u0026gt;% dplyr::select(id, class, estimate) %\u0026gt;% spread(class, estimate) %\u0026gt;% dplyr::select(-id)  We can apply the mean function using purrr::map():\ntaxa.mean \u0026lt;- purrr::map(taxa.slopes, ~mean(., na.rm = TRUE)) # This plots the mean population change per taxa taxa.mean  Now we can write our own function to make histograms and use the purrr package to apply it to each taxa.\n### Intro to the purrr package ---- # First let's write a function to make the plots # *** Functional Programming *** # This function takes one argument x, the data vector that we want to make a histogram plot.hist \u0026lt;- function(x) { ggplot() + geom_histogram(aes(x), colour = \u0026quot;darkgreen\u0026quot;, fill = \u0026quot;darkgreen\u0026quot;, binwidth = 0.02) + theme_LPD() + xlab(\u0026quot;Rate of population change (slopes)\u0026quot;) }  Now we can use purr to \u0026ldquo;map\u0026rdquo; our figure making function. The first input is your data that you want to iterate over and the second input is the function.\ntaxa.plots \u0026lt;- purrr::map(taxa.slopes, ~plot.hist(.)) # We need to make a new folder to put these figures in path2 \u0026lt;- \u0026quot;Taxa_Forest_LPD_purrr/\u0026quot; dir.create(path2)  First we learned about map() when there is one dataset, but there are other purrr functions,too. walk2() takes two arguments and returns nothing. In our case we just want to print the graphs, so we don\u0026rsquo;t need anything returned. The first argument is our file path, the second is our data and ggsave is our function.\n# *** walk2() function in purrr from the tidyverse *** walk2(paste0(path2, names(taxa.slopes), \u0026quot;.pdf\u0026quot;), taxa.plots, ggsave)  \n3. Downloading and mapping data from large datasets Map the distribution of a forest vertebrate species and the location of monitored populations  How to download GBIF records How to map occurence data and populations How to make a custom function for plotting figures  In this part of the tutorial, we will focus on one particular species, red deer (Cervus elaphus), where it has been recorded around the world, and where it\u0026rsquo;s populations are being monitored. We will use occurrence data from the Global Biodiversity Information Facility which we will download in R using the rgbif package.\nOccurrence data can be messy and when you are working with thousands of records, not all of them might be valid records. If you are keen to find out how to test the validity of geographic coordinates using the CoordinateCleaner package, check out our tutorial here.\n### PART 3: Downloading and mapping data from large datasets ---- #### How to map distributions and monitoring locations for one or more taxa # Packages ---- library(rgbif) # To extract GBIF data # library(CoordinateCleaner) # To clean coordinates if you want to explore that later library(gridExtra) # To make pretty graphs library(ggrepel) # To add labels with rounded edges library(png) # To add icons library(mapdata) # To plot maps library(ggthemes) # To make maps extra pretty  We are limiting the number of records to 5000 for the sake of time - in the future you can ask for more records as well, there\u0026rsquo;s just a bit of waiting involved. The records come with a lot of metadata. For our purposes, we will select just the columns we need. Similar to how before we had to specify that we want the map() function from the purrr package, there are often other select() functions, so we are saying that we want the one from dplyr using dplyr::select(). Otherwise, the select() function might not work because of a conflict with another select() function from a different package, e.g. the raster packge.\n# Download species occurrence records from the Global Biodiversity Information Facility # *** rgbif package and the occ_search() function *** # You can increase the limit to get more records - 5000 takes a couple of minutes deer.locations \u0026lt;- occ_search(scientificName = \u0026quot;Cervus elaphus\u0026quot;, limit = 5000, hasCoordinate = TRUE, return = \u0026quot;data\u0026quot;) %\u0026gt;% # Simplify occurrence data frame dplyr::select(key, name, decimalLongitude, decimalLatitude, year, individualCount, country)  Next we will extract the red deer population data - the raw time series and the slopes of population change from the two data frames.\n# Data formatting \u0026amp; manipulation ---- # Filter out population data for chosen species - red deer deer.data \u0026lt;- LPD_long2 %\u0026gt;% filter(species.name == \u0026quot;Cervus elaphus\u0026quot;) %\u0026gt;% dplyr::select(id, species.name, location.of.population, year, pop) # Filter out population estimates for chosen species - red deer deer.slopes \u0026lt;- forest.slopes %\u0026gt;% filter(species.name == \u0026quot;Cervus elaphus\u0026quot;)  In addition to making histograms, scatterplots and such, you can use ggplot2 to make maps as well - the maps come from the mapdata package we loaded earlier. In this map, we want to visualise information from two separate data frames - where the species occurs (deer.locations, the GBIF data) and where it is monitored (deer.slopes, the Living Planet Database data). We can combine this information in the same ggplot2 code chunk using the geom_point() function twice - the first time it will plot the occurrences since that the data frame associated with the plot in the very first line, and the second time we\u0026rsquo;ve told the function to specifically use the deer.slopes object using data = deer.slopes.\nAs you start making your maps, you may get this warning message:\nWarning message: In drawGrob(x) : reached elapsed time limit  We are working with thousands of records, so depending on your computer, making the map might take a while. This message doesn\u0026rsquo;t mean something is wrong, just lets you know that generating the map took a bit longer than what RStudio expected.\n# Make an occurrence map and include the locations of the populations part of the Living Planet Database (deer.map.LPD \u0026lt;- ggplot(deer.locations, aes(x = decimalLongitude, y = decimalLatitude)) + # Add map data borders(\u0026quot;world\u0026quot;, colour = \u0026quot;gray80\u0026quot;, fill = \u0026quot;gray80\u0026quot;, size = 0.3) + # Use custom map theme from ggthemes package theme_map() + # Add the points from the population change data geom_point(alpha = 0.3, size = 2, colour = \u0026quot;aquamarine3\u0026quot;) + # Specify where the data come from when plotting from more than one data frame using data = \u0026quot;\u0026quot; geom_point(data = deer.slopes, aes(x = decimal.longitude, y = decimal.latitude), size = 2, colour = \u0026quot;darkgreen\u0026quot;))   \nThe map already looks fine, but we can customise it further to add more information. For example, we can add labels for the locations of some of the monitored populations and we can add plots of population change next to our map.\nFirst we will rename some of the populations, just so that our labels are not crazy long, using the recode() function from the dplyr package.\n# Customising map to make it more beautiful ---- # Check site names print(deer.slopes$location.of.population) # Beautify site names deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Northern Yellowstone National Park\u0026quot; = \u0026quot;Yellowstone National Park\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Mount Rainier National Park, USA\u0026quot; = \u0026quot;Mount Rainier National Park\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Bow Valley - eastern zone, Banff National Park, Alberta\u0026quot; = \u0026quot;Banff National Park, Alberta\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Bow Valley - western zone, Banff National Park, Alberta\u0026quot; = \u0026quot;Banff National Park, Alberta\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Bow Valley - central zone, Banff National Park, Alberta\u0026quot; = \u0026quot;Banff National Park, Alberta\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Study area within Bow Valley, Banff National Park, Alberta\u0026quot; = \u0026quot;Banff National Park, Alberta\u0026quot;) deer.slopes$location.of.population \u0026lt;- recode(deer.slopes$location.of.population, \u0026quot;Bow Valley watershed of Banff National Park, Alberta\u0026quot; = \u0026quot;Banff National Park, Alberta\u0026quot;)  You can also use ggplot2 to add images to your graphs, so here we will add a deer icon.\n# Load packages for adding images packs \u0026lt;- c(\u0026quot;png\u0026quot;,\u0026quot;grid\u0026quot;) lapply(packs, require, character.only = TRUE) # Load red deer icon icon \u0026lt;- readPNG(\u0026quot;reddeer.png\u0026quot;) icon \u0026lt;- rasterGrob(icon, interpolate = TRUE)  We can update our map by adding labels and our icon - this looks like a gigantic chunk of code, but we\u0026rsquo;ve added line by line comments so that you can see what\u0026rsquo;s happening at each step. The ggrepel package adds labels whilst also aiming to avoid overlap and as a bonus, the labels have rounded edges.\n# Update map # Note - this takes a while depending on your computer (deer.map.final \u0026lt;- ggplot(deer.locations, aes(x = decimalLongitude, y = decimalLatitude)) + # For more localized maps use \u0026quot;worldHires\u0026quot; instead of \u0026quot;world\u0026quot; borders(\u0026quot;world\u0026quot;, colour = \u0026quot;gray80\u0026quot;, fill = \u0026quot;gray80\u0026quot;, size = 0.3) + theme_map() + geom_point(alpha = 0.3, size = 2, colour = \u0026quot;aquamarine3\u0026quot;) + # We are specifying the data frame for the labels - one site has three monitored populations # but we only want to label it once so we are subsetting using data = deer.slopes[c(2, 4, 5, 9),] # to get only the first rows number 2, 4, 5 and 9 geom_label_repel(data = deer.slopes[c(2, 4, 5, 9),], aes(x = decimal.longitude, y = decimal.latitude, label = location.of.population), box.padding = 1, size = 5, nudge_x = 1, # We are specifying the size of the labels and nudging the points so that they # don't hide data points, along the x axis we are nudging by one min.segment.length = 0, inherit.aes = FALSE) + # We can recreate the shape of a dropped pin by overlaying a circle and a triangle geom_point(data = deer.slopes, aes(x = decimal.longitude, y = decimal.latitude + 0.6), size = 4, colour = \u0026quot;darkgreen\u0026quot;) + geom_point(data = deer.slopes, aes(x = decimal.longitude, y = decimal.latitude - 0.3), size = 3, fill = \u0026quot;darkgreen\u0026quot;, colour = \u0026quot;darkgreen\u0026quot;, shape = 25) + # Adding the icon using the coordinates on the x and y axis annotation_custom(icon, xmin = -210, xmax = -100, ymin = -60 , ymax = -30) + # Adding a title labs(title = \u0026quot;a. Red Deer GBIF occurrences\u0026quot;, size = 12))  Let\u0026rsquo;s add some additional plots to our figure, for example how many occurrences there are for each year.\n# Visualise the number of occurrence records through time ---- # This plot is more impressive if you have downloaded more records # as GBIF downloads the most recent records first yearly.obs \u0026lt;- deer.locations %\u0026gt;% group_by(year) %\u0026gt;% tally() %\u0026gt;% ungroup() %\u0026gt;% filter(is.na(year) == FALSE) (occurrences \u0026lt;- ggplot(yearly.obs, aes(x = year, y = n)) + geom_smooth(colour = \u0026quot;aquamarine3\u0026quot;, method = 'loess', size = 1) + labs(x = NULL, y = \u0026quot;Number of occurrences\\n\u0026quot;, title = \u0026quot;b. GBIF occurrences\\n\u0026quot;, size = 12) + # Use our customised theme, saves many lines of code! theme_LPD() + # if you want to change things about your theme, you need to include the changes after adding the theme theme(plot.title = element_text(size = 12), axis.title.y = element_text(size = 10)))  We can add plots that show the population trends for those populations we\u0026rsquo;ve labelled. Given that we will be doing the same thing for multiple objects (the same type of plot for each population), we can practice functional programming and using purrr again here. The function looks very similar to a normal ggplot2 code chunk, except we\u0026rsquo;ve wrapped it up in a function and we are not using any specific objects, just x, y and z as the three arguments the function needs.\n# Visualise population trends ---- # Visualising the population trends of four deer populations # Let's practice functional programming here # *** Functional Programming *** # Let's make a function to make the population trend plots # First we need to decide what values the function needs to take # x - The population data # y - the slope value # z - the location of the monitoring # This function needs to take three arguments # Let's make the ggplot function pop.graph \u0026lt;- function(x, y, z) { # Make a ggplot graph with the 'x' ggplot(x, aes(x = year, y = pop)) + # Shape 21 chooses a point with a black outline filled with aquamarine geom_point(shape = 21, fill = \u0026quot;aquamarine3\u0026quot;, size = 2) + # Adds a linear model fit, alpha controls the transparency of the confidence intervals geom_smooth(method = \u0026quot;lm\u0026quot;, colour = \u0026quot;aquamarine3\u0026quot;, fill = \u0026quot;aquamarine3\u0026quot;, alpha = 0.4) + # Add the monitoring location 'y' into the plot labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Individuals\\n\u0026quot;, title = paste(\u0026quot;c. \u0026quot;, y, \u0026quot;\\n\u0026quot;), size = 7) + # Set the y limit to the maximum population for each 'x' ylim(0, max(x$pop)) + # Set the x limit to the range of years of data xlim(1970, 2010) + # Add the slope 'y' into the plot annotate(\u0026quot;text\u0026quot;, x = 1972, y = 0, hjust = 0, vjust = -2, label = paste(\u0026quot;Slope =\u0026quot;, z), size = 3) + theme_LPD() + theme(plot.title = element_text(size=12), axis.title.y = element_text(size=10)) }  We will focus on four populations in the USA, Switzerland and Canada. We will make three objects for each population, which represent the three arguments the function takes - the population data, the slope value and the location of the population. Then we will run our function pop.graph() using those objects.\n# Find all unique ids for red deer populations unique(deer.slopes$id) # Create an object each of the unique populations # Deer population 1 - Northern Yellowstone National Park deer1 \u0026lt;- filter(deer.data, id == \u0026quot;6395\u0026quot;) slope_deer1 \u0026lt;- round(deer.slopes$estimate[deer.slopes$id == \u0026quot;6395\u0026quot;],2) location_deer1 \u0026lt;- deer.slopes$location.of.population[deer.slopes$id == \u0026quot;6395\u0026quot;] yellowstone \u0026lt;- pop.graph(deer1, location_deer1, slope_deer1) # Deer population 2 - Mount Rainier National Park, USA deer2 \u0026lt;- filter(deer.data, id == \u0026quot;3425\u0026quot;) slope_deer2 \u0026lt;- round(deer.slopes$estimate[deer.slopes$id == \u0026quot;3425\u0026quot;],2) location_deer2 \u0026lt;- deer.slopes$location.of.population[deer.slopes$id == \u0026quot;3425\u0026quot;] rainier \u0026lt;- pop.graph(deer2, location_deer2, slope_deer2) # Deer population 3 - Switzerland deer3 \u0026lt;- filter(deer.data, id == \u0026quot;11170\u0026quot;) slope_deer3 \u0026lt;- round(deer.slopes$estimate[deer.slopes$id == \u0026quot;11170\u0026quot;],2) location_deer3 \u0026lt;- deer.slopes$location.of.population[deer.slopes$id == \u0026quot;11170\u0026quot;] switzerland \u0026lt;- pop.graph(deer3, location_deer3, slope_deer3) # Deer population 4 - Banff National Park, Alberta (there are more populations here) deer4 \u0026lt;- filter(deer.data, id == \u0026quot;4383\u0026quot;) slope_deer4 \u0026lt;- round(deer.slopes$estimate[deer.slopes$id == \u0026quot;4383\u0026quot;],2) location_deer4 \u0026lt;- deer.slopes$location.of.population[deer.slopes$id == \u0026quot;4383\u0026quot;] banff \u0026lt;- pop.graph(deer4, location_deer4, slope_deer4)  We are now ready to combine all of our graphs into one panel. When using grid.arrange(), you can add an additional argument widths = c() or heights = c(), which controls the ratios between the different plots. By default, grid.arrange() will give equal space to each graph, but sometimes you might want one graph to be wider than others, like here we want the map to have more space.\n# Create panel of all graphs # Makes a panel of the map and occurrence plot and specifies the ratio # i.e., we want the map to be wider than the other plots # suppressWarnings() suppresses warnings in the ggplot call here row1 \u0026lt;- suppressWarnings(grid.arrange(deer.map.final, occurrences, ncol = 2, widths = c(1.96, 1.04))) # Makes a panel of the four population plots row2 \u0026lt;- grid.arrange(yellowstone, rainier, switzerland, banff, ncol = 4, widths = c(1, 1, 1, 1)) # Makes a panel of all the population plots and sets the ratio # Stich all of your plots together deer.panel \u0026lt;- grid.arrange(row1, row2, nrow = 2, heights = c(1.2, 0.8)) ggsave(deer.panel, filename = \u0026quot;deer_panel2.png\u0026quot;, height = 10, width = 15)   \nChallenges Take what you have learned about pipes and make a map of the five most well-sampled populations in the LPD database (the ones with the most replicate populations) and colour code the points by the population trend (derived from the models we did) and the size by the duration of the time series. You can try incorporating a handwritten function to make the map and using purr to implement that function, or you can go straight into ggplot2.\nPick a country and species of your choice. Download the GBIF records for that species from your selected country (or you can do the world if you don\u0026rsquo;t mind waiting a few more minutes for the GBIF data to download). Plot where the species occurs. Then, add the locations of the Living Planet Database populations of the same species - do we have long-term records from the whole range of the species? Where are the gaps? You can have a go at combining the LPD and GBIF databases in a meaningful way - hint: look up the different joining functions from dplyr - left_join(), inner_join(), etc.\nUse another projection for the map - the default is Mercator, but that\u0026rsquo;s not the best way to represent the world. Hint - you can still use ggplot2 - look up the proj4 package and how to combine it with ggplot2.\nExtra resources You can find more info on pander here.\nTo learn more about the power of pipes check out: the tidyverse website and the R for Data Science book.\nTo learn more about purrr check out the tidyverse website and the R for Data Science book.\nFor more information on functional programming see the R for Data Science book chapter here.\nTo learn more about the tidyverse in general, check out Charlotte Wickham\u0026rsquo;s slides here.\n  \n\u0026nbsp; Subscribe to the coding club mailing list:   Subscribe     \n  \u0026nbsp;Follow our coding adventures on Twitter!     ","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"16c060204fb287ac5cdeca2987144015","permalink":"https://BES-QSIG.github.io/courses/advancingr-tidyverse/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/courses/advancingr-tidyverse/","section":"courses","summary":"PART I: Create a reproducible report using Markdown 1. What is R Markdown? R Markdown allows you to create documents that serve as a neat record of your analysis. In the world of reproducible research, we want other researchers to easily understand what we did in our analysis. You might choose to create an R markdown document as an appendix to a paper or project assignment that you are doing, upload it to an online repository such as Github, or simply to keep as a personal record so you can quickly look back at your code and see what you did.","tags":null,"title":"Day One: Data manipulation and Visualisation using the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":" Part I: Preliminaries Required packages For this \u0026ldquo;tutorial\u0026rdquo;, you will need some packages and their dependencies\nif(!requireNamespace(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;) if(!requireNamespace(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;) if(!requireNamespace(\u0026quot;ggplot2\u0026quot;)) install.packages(\u0026quot;ggplot2\u0026quot;) if(!requireNamespace(\u0026quot;ggspatial\u0026quot;)) install.packages(\u0026quot;ggspatial\u0026quot;) if(!requireNamespace(\u0026quot;rgdal\u0026quot;)) install.packages(\u0026quot;rgdal\u0026quot;) if(!requireNamespace(\u0026quot;RPostgreSQL\u0026quot;)) install.packages(\u0026quot;RPostgreSQL\u0026quot;) if(!requireNamespace(\u0026quot;rgbif\u0026quot;)) install.packages(\u0026quot;rgbif\u0026quot;) library(raster) library(sf) library(ggplot2) library(ggspatial) library(RPostgreSQL) library(rgdal) library(rgbif)  The data for this tutorial are available from here.\nWhat is a spatial object? A spatial object is an entity with coordinates in a geographical space (x, y) or (x, y, z) with a specific projection.\n Coordinates are not enough! To illustrate this affirmation, just have a look at this example.\n # create two ojbects with their long/lat coordinates point_liverpool \u0026lt;- data.frame(name = 'Liverpool', longitude = -2.98, latitude = 53.41) point_edinburgh \u0026lt;- data.frame(name = 'Edinburgh', longitude = -3.19, latitude = 55.95) city_points \u0026lt;- rbind(point_liverpool, point_edinburgh) # Now have a look at them in a 2D space. plot(city_points[,c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;)], pch = 19, col = 'magenta') text(city_points[,c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;)], labels = city_points$name, pos = c(2, 4))  With these two cities, although the coordinates are right, you have no information about the context of their coordinates.\n Spatial projection The Coordinate Reference System or CRS of a spatial object tells R where the spatial object is located in geographic space (see http://spatialreference.org/).\n For example, you might have seen the expression WGS84, the common longitude-latitude (degree decimal), or maybe EPSG:4326. In other words, spatial objects should come with some information relative to their reference system (CRS), no matter if it\u0026rsquo;s a raster, a point, a line of a complex polygon. To locate an object in space, you need to know the reference system and the units in which the coordinates are expressed.\n# Adding a projection to our cities, using WGS84 proj_city_points \u0026lt;- sf::st_as_sf(city_points, coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;), crs = 4326) plot(proj_city_points, pch = 19, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), cex = 1.5) legend(\u0026quot;topleft\u0026quot;, legend = proj_city_points$name, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), pch = 19, cex = 1.5, bty=\u0026quot;n\u0026quot;) # get some base layer to see this in perspective country_sf_gbr \u0026lt;- sf::st_as_sf(raster::getData(name = \u0026quot;GADM\u0026quot;, country = 'GBR', level = 1)) # visualize the points on a map plot(country_sf_gbr$geometry, graticule = TRUE, axes = TRUE, col = \u0026quot;wheat1\u0026quot;, xlim = c(-15, 5), ylim = c(50, 61)) plot(proj_city_points, pch = 19, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), cex = 1.5, add = TRUE) legend(\u0026quot;topleft\u0026quot;, legend = proj_city_points$name, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), pch = 19, cex = 1.5, bty=\u0026quot;n\u0026quot;) text(x = -15, y = 50.2, \u0026quot;EPSG:4326 - WGS84\u0026quot;, pos = 4, cex = 0.7)  The advantage of having a CRS is that you can transform your coordinates, so it is expressed to other reference system (i.e. on other origin and units).\n Changing Projection (st_tranform) Some projections more appropriate for representing specific geographic context. Why is their so many CRS?\n # Reproject your city points in OSGB 1936 / British National Grid (EPSG:27700) proj_city_points_osgb \u0026lt;- sf::st_transform(proj_city_points, crs = 27700) country_sf_gbr_osgb \u0026lt;- sf::st_transform(country_sf_gbr, crs = 27700) # visualize the points on a map dev.new() plot(country_sf_gbr_osgb$geometry, graticule = TRUE, axes = TRUE, col = \u0026quot;wheat1\u0026quot;, xlim = c(-333585, 713000), ylim = c(20000, 1290000)) plot(proj_city_points_osgb, pch = 19, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), cex = 1.5, add = TRUE) legend(\u0026quot;topleft\u0026quot;, legend = proj_city_points_osgb$name, col = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;), pch = 19, cex = 1.5, bty=\u0026quot;n\u0026quot;) text(x = -453585, y = 25000, \u0026quot;EPSG:27700 - OSGB 1936 \u0026quot;, pos = 4, cex = 0.7)   to get coordinates on a plot, you can use the function drawExtent() from the raster package that allow you to clic on a map to get the coordinates of an extent.\n plot(country_sf_gbr_osgb$geometry, graticule = TRUE, axes = TRUE, col = \u0026quot;wheat1\u0026quot;) raster::drawExtent()  PART II: Building and working with spatial objects using sf in R This is a revolution, providing a modern, stronger and cleaner workflow to deal with spatial object in R, at least vector data. The \u0026ldquo;sf\u0026rdquo; is developed by some of the same people that provide us with \u0026ldquo;sp\u0026rdquo;, offering an ecosystem that open new opportunities to do GIS in R. The firs place to look for resource is the sf package website https://r-spatial.github.io/sf/index.html, this a first stop to learn how it work and develop new skills in R.\nlibrary(sf) p1 \u0026lt;- sf::st_point(c(1, 2)) p2 \u0026lt;- sf::st_point(c(3, 5)) p3 \u0026lt;- sf::st_multipoint(matrix(2 * c(1, 2, 4, 2, 3, 5, 7, 3), ncol = 2, byrow = TRUE)) p4 \u0026lt;- sf::st_as_sf(data.frame(X = c(1, 4, 3, 7), Y = c(2, 2, 5, 3) ), coords = c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;)) p5 \u0026lt;- sf::st_sfc(p1, p2, p3) plot(p1) plot(p3, col = \u0026quot;blue\u0026quot;, pch = 19) plot(p2, col = \u0026quot;magenta\u0026quot;, pch = 19, add = TRUE)  Modify sf - sfc objects p6 \u0026lt;- sf::st_cast(x = st_sfc(p3), to = \u0026quot;POINT\u0026quot;) p_multi \u0026lt;- sf::st_cast(p6, ids = c(1, 2, 1, 2), group_or_plist = TRUE, to = \u0026quot;MULTIPOINT\u0026quot;) plot(p_multi[1], ylim = c(0, 20), xlim = c(0, 20), col = \u0026quot;tomato3\u0026quot;, pch = 19) plot(p_multi[2], col = \u0026quot;magenta\u0026quot;, pch = 19, add = TRUE) p7 \u0026lt;- st_cast(x = p4, to = \u0026quot;POINT\u0026quot;) p8 \u0026lt;- rbind(st_cast(x = p4[1:3,], to = \u0026quot;MULTIPOINT\u0026quot;), p4[4,]) p8  Lines l1 \u0026lt;- sf::st_linestring(matrix(c(1, 1, 2, 2, 3, 3, 4, 4, 4, 2), ncol = 2, byrow = TRUE)) lp \u0026lt;- sf::st_cast(x = sf::st_sfc(l1), to = \u0026quot;MULTIPOINT\u0026quot;) bl1 \u0026lt;- sf::st_buffer(l1, 2) blp \u0026lt;- sf::st_cast(x = sf::st_sfc(bl1), to = \u0026quot;MULTIPOINT\u0026quot;) plot(lp, col = \u0026quot;blue\u0026quot;, pch = 19, cex = 2, ylim = c(-5, 10), xlim = c(-5, 10)) plot(blp, col = \u0026quot;magenta\u0026quot;, pch = 19, cex = 1, add = TRUE) plot(l1, col = \u0026quot;tomato3\u0026quot;, lwd = 1.5, add = TRUE)  Polygon bl1 \u0026lt;- sf::st_buffer(l1, 2) blp \u0026lt;- sf::st_cast(x = sf::st_sfc(bl1), to = \u0026quot;MULTIPOINT\u0026quot;) plot(bl1, col = \u0026quot;lightblue\u0026quot;, border = NA) plot(lp, col = \u0026quot;blue\u0026quot;, pch = 19, cex = 2, ylim = c(-5, 10), xlim = c(-5, 10), add = TRUE) plot(blp, col = \u0026quot;magenta\u0026quot;, pch = 19, cex = 1, add = TRUE) plot(l1, col = \u0026quot;tomato3\u0026quot;, lwd = 1.5, add = TRUE)  Intersects and intesections p1 \u0026lt;- sf::st_as_sf(data.frame(X = c(1, 4, 3, 7), Y = c(2, 2, 5, 3) ), coords = c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;), crs = 4326) poly1 \u0026lt;- st_as_sfc(st_bbox(st_buffer(p1[2,], 2))) poly2 \u0026lt;- st_as_sfc(st_bbox(st_buffer(p1[3,], 1.5))) plot(st_geometry(poly1), col = \u0026quot;goldenrod\u0026quot;, xlim = c(-5, 5), ylim = c(0, 10)) plot(st_geometry(poly2), col = rgb(1,1,0,0.3), add = TRUE) plot(st_geometry(p1), col = \u0026quot;magenta\u0026quot;, pch = 19, cex= 1.5, add = TRUE) ## INTERSECTION poly3 \u0026lt;- sf::st_intersection(poly1, poly2) plot(st_geometry(poly3), col = \u0026quot;lightblue\u0026quot;, add = TRUE) ## INTERSECT p1_poly1 \u0026lt;- sf::st_intersects(p1, poly1, sparse = FALSE) plot(st_geometry(p1[p1_poly1,]), col = \u0026quot;turquoise\u0026quot;, pch = 19, cex = 2, add = TRUE)  circle buffer intersection poly1 \u0026lt;- sf::st_buffer(p1[2,], 2) poly2 \u0026lt;- sf::st_buffer(p1[3,], 1.5) int_b2_b1 \u0026lt;- sf::st_intersection(poly2, poly1) plot(st_geometry(poly1), col = NA, border = \u0026quot;red\u0026quot;, ylim = c(0, 7), axes = TRUE) plot(st_geometry(poly2), add = TRUE) plot(st_geometry(p1[2,]), col = \u0026quot;black\u0026quot;, pch = 19, add = TRUE) plot(st_geometry(p1[3,]), col = \u0026quot;blue\u0026quot;, pch = 19, add = TRUE) plot(st_geometry(int_b2_b1), col = \u0026quot;lightblue\u0026quot;, boder = NA, add = TRUE)  Difference between objects poly1 \u0026lt;- sf::st_buffer(p1[2,], 2) poly2 \u0026lt;- sf::st_buffer(p1[2,], 4) dif_poly2_poly1 \u0026lt;- sf::st_difference(poly2, poly1) plot(st_geometry(dif_poly2_poly1), col = \u0026quot;orange\u0026quot;, axes = TRUE) plot(st_geometry(poly1), col = \u0026quot;blue\u0026quot;, add = TRUE)  Union (merge and melt) objects poly1 \u0026lt;- sf::st_buffer(p1[2,], 2) poly2 \u0026lt;- sf::st_buffer(p1[2,], 4) uni_poly1_2 \u0026lt;- sf::st_union(dif_poly2_poly1, poly1) plot(st_geometry(uni_poly1_2), col = \u0026quot;lightblue\u0026quot;, axes = TRUE) plot(st_geometry(poly1), col = \u0026quot;tomato3\u0026quot;, add = TRUE) plot(st_geometry(st_buffer(poly1,-1)), col = \u0026quot;white\u0026quot;, add = TRUE) plot(st_geometry(st_centroid(poly1)), col = \u0026quot;black\u0026quot;, pch = 19, add = TRUE)  If you like it tidy and the dplyr way? Since sf is essentally a data.frame with a list of spatial attribute attached, it works well in the tidy univers. Have a look at this blog to get a first feel http://strimas.com/r/tidy-sf/.\nPART III: Load and manipulate spatial objects Spatial data are increasingly available from the Web, from species occurrence to natural and cultural features data, accessing spatial data is now relatively easy. For base layers, you can find many freely available data sets such as the ones provided by the Natural Earth [http://www.naturalearthdata.com], the IUCN Protected Planet database [www.protectedplanet.net], the GADM project [https://gadm.org], worldclim [http://worldclim.org/version2] the CHELSA climate data sets [http://chelsa-climate.org] or the European Environmental Agency [https://www.eea.europa.eu/data-and-maps/data#c0=5\u0026amp;c11=\u0026amp;c5=all\u0026amp;b_start=0]\nRaster object library(raster) # annual mean temperature chelsa_amt \u0026lt;- raster::raster(\u0026quot;data/CHELSA_bio10_1.tif\u0026quot;) raster::plot(chelsa_amt) chelsa_amt # crop climate data for a specific bounding box chelsa_amt_gbr \u0026lt;- raster::crop(chelsa_amt, raster::extent(country_sf_gbr)) # cut the worldwide raster according to raster::plot(chelsa_amt_gbr) # convert temperature data in usual unit chelsa_amt_gbr[] \u0026lt;- chelsa_amt_gbr[]*0.1 # convert temperature data in usual unit raster::res(chelsa_amt_gbr) # change resolution aggregate() or disaggregate() chelsa_amt_gbr_2 \u0026lt;- raster::disaggregate(chelsa_amt_gbr, fact=4) raster::res(chelsa_amt_gbr_2) chelsa_amt_gbr_3 \u0026lt;- raster::aggregate(chelsa_amt_gbr, fact=4) raster::res(chelsa_amt_gbr_3) raster::plot(chelsa_amt_gbr_3)   Other format for gridded data netCDF format is commonly used for gridded time series (temperature)\n rast_1 \u0026lt;- raster::raster(\u0026quot;data/tg_0.25deg_reg_2011-2017_v17.0.nc\u0026quot;, band = 1) rast_2 \u0026lt;- raster::raster(\u0026quot;data/tg_0.25deg_reg_2011-2017_v17.0.nc\u0026quot;, band = 2) rast_stack \u0026lt;- raster::stack(rast_1, rast_2) plot(rast_stack$mean.temperature.1) plot(rast_stack)  Land cover eunis_1km \u0026lt;- raster::raster(\u0026quot;data/es_l1_1km.tif\u0026quot;) raster::plot(eunis_1km) raster::projection(eunis_1km)  Spatial extraction (raster value for a vector object) proj_city_points_laea \u0026lt;- sf::st_transform(proj_city_points, raster::projection(eunis_1km)) eunis_city \u0026lt;- raster::extract(eunis_1km, as(proj_city_points_laea, \u0026quot;Spatial\u0026quot;)) eunis_city \u0026lt;- raster::extract(eunis_1km, as(sf::st_buffer(proj_city_points_laea, 10000), \u0026quot;Spatial\u0026quot;)) str(eunis_city) table(eunis_city[[1]]) proportion \u0026lt;- as.numeric(table(eunis_city[[1]]))[which(names(table(eunis_city[[1]]))!=\u0026quot;10\u0026quot;)] / sum(as.numeric(table(eunis_city[[1]]))) * 100 proportion  Vector object library(sf) st_prov \u0026lt;- sf::st_read(\u0026quot;data/GADM_2.8_GBR_adm2.shp\u0026quot;) plot(st_prov[,\u0026quot;HASC_2\u0026quot;], graticule = TRUE, axes = TRUE) # Older option library(rgdal) st_prov_sp \u0026lt;- rgdal::readOGR(\u0026quot;data\u0026quot;, \u0026quot;GADM_2.8_GBR_adm2\u0026quot;) class(st_prov_sp) plot(st_prov_sp, axes = TRUE) # Change projection of an \u0026quot;sp\u0026quot; object crs.osgb = CRS(\u0026quot;+init=epsg:27700\u0026quot;) st_prov_sp.osgb = sp::spTransform(st_prov_sp, crs.osgb) plot(st_prov_sp.osgb, axes = TRUE) spplot(st_prov_sp.osgb, \u0026quot;HASC_2\u0026quot;, colorkey = FALSE)  Read and write your spatial object in Shapefile # read with \u0026quot;sf\u0026quot; st_prov \u0026lt;- sf::st_read(\u0026quot;data/GADM_2.8_GBR_adm2.shp\u0026quot;) # write with \u0026quot;sf\u0026quot; sf::st_write(st_prov, dsn = \u0026quot;st_prov.shp\u0026quot;, delete_layer = TRUE) # write with OGR (rgdal) st_prov_sp \u0026lt;- sf::as(st_prov, \u0026quot;Spatial\u0026quot;) rgdal::writeOGR(st_prov_sp, \u0026quot;st_prov_sp\u0026quot;, driver = \u0026quot;ESRI Shapefile\u0026quot;)  Subseting and filtering wdpa_gbr \u0026lt;- sf::st_read(\u0026quot;data/wdpa_gbr.shp\u0026quot;) wdpa_gbr \u0026lt;- wdpa_gbr[wdpa_gbr$MARINE == 0,] plot(wdpa_gbr$geometry) ## bounding box bb \u0026lt;- sf::st_bbox(country_sf_gbr) bb ## faster solution bb_poly \u0026lt;- sf::st_make_grid(country_sf_gbr, n = 1) wdpa_gbr_2 \u0026lt;- sf::st_intersects(wdpa_gbr, bb_poly, sparse = FALSE) wdpa_gbr \u0026lt;- wdpa_gbr[wdpa_gbr_2,] plot(wdpa_gbr$geometry[wdpa_gbr$IUCN_CAT == \u0026quot;V\u0026quot;]) wdpa_cntr \u0026lt;- sf::st_centroid(wdpa_gbr) plot(wdpa_cntr$geometry) ## building a box from the st_bbox output g.bbox \u0026lt;- raster::extent(as.numeric(sf::st_bbox(country_sf_gbr))[c(1,3,2,4)]) g.bbox_sf \u0026lt;- sf::st_set_crs(sf::st_as_sfc(as(g.bbox, 'SpatialPolygons')), 3035) plot(g.bbox_sf, add = TRUE)   Try it your self Get some river data from http://land.copernicus.eu/pan-european/satellite-derived-products/eu-hydro/eu-hydro-public-beta/eu-hydro-river-network/view\n PART IV: Interfacing R and PosgreSQL/PostGIS  Install PostgreSQL, with the PostGIS extension Create a database Populate your database Extract from your database  Create PostgreSQL database library('RPostgreSQL') # create a new database in on your local PostgreSQL server sql_createdb \u0026lt;- paste0(\u0026quot;-h localhost -U \u0026quot;, \u0026quot;postgres\u0026quot;, \u0026quot; -T \u0026quot;, \u0026quot;postgis_22_sample\u0026quot;, \u0026quot; -E UTF8 -O postgres \u0026quot;, \u0026quot;RGIS_workshop\u0026quot;) system2(\u0026quot;createdb\u0026quot;, sql_createdb, invisible = FALSE) # connect R to your server and newly created database drv \u0026lt;- dbDriver(\u0026quot;PostgreSQL\u0026quot;) dbcon \u0026lt;- dbConnect(drv, dbname = \u0026quot;RGIS_workshop\u0026quot;, host = \u0026quot;localhost\u0026quot;, port = 5432, user = \u0026quot;postgres\u0026quot;, password = \u0026quot;****\u0026quot;) # create a schema, send your first SQL statement (query) sql_createschema \u0026lt;- paste0(\u0026quot;CREATE SCHEMA IF NOT EXISTS my_shemas AUTHORIZATION postgres;\u0026quot;) dbSendStatement(dbcon, sql_createschema) # have a look at your PostgreSQL database, using pgAdmin  Send spatial data to your newly created database located on your local server library(sf) dbcon \u0026lt;- dbConnect(drv, dbname = \u0026quot;RGIS_workshop\u0026quot;, host = \u0026quot;localhost\u0026quot;, port = 5432, user = \u0026quot;postgres\u0026quot;, password = \u0026quot;*****\u0026quot;) wdpa_gbr \u0026lt;- sf::st_read(\u0026quot;data/wdpa_gbr.shp\u0026quot;) sf::st_write(wdpa_gbr, dbcon, overwrite = TRUE) # check if you succeeded dbExistsTable(dbcon, \u0026quot;wdpa_gbr\u0026quot;) # extract some data with a SQL query and a condition psql_extract \u0026lt;- dbGetQuery(dbcon, \u0026quot;SELECT \\\u0026quot;IUCN_CAT\\\u0026quot;, ST_AsText(geometry) as geom FROM wdpa_gbr WHERE \\\u0026quot;IUCN_CAT\\\u0026quot; = \\'V\\' AND \\\u0026quot;MARINE\\\u0026quot; = \\'0\\'\u0026quot;) str(psql_extract) new_extract \u0026lt;- sf::st_as_sf(psql_extract, wkt = \u0026quot;geom\u0026quot;) new_extract \u0026lt;- sf::st_set_crs(new_extract, 4326) plot(new_extract) new_area \u0026lt;- sf::st_area(sf::st_transform(new_extract, 27700)) head(new_area)  Interacting with PostgreSQL through your terminal  In your terminal, type ogr2ogr -f \u0026ldquo;PostgreSQL\u0026rdquo; -t_srs EPSG:27700 PG:\u0026ldquo;host=localhost port=5432 dbname=RGIS_workshop user=postgres password=****\u0026rdquo; \u0026lsquo;C:\\Users\\retoschm\\OneDrive - Natural Environment Research Council\\Rgis_workshop\\data\\GADM_2.8_GBR_adm2.shp\u0026rsquo; -nln public.wdpa_gbr2 -nlt MULTIPOLYGON -overwrite -progress -unsetFid \u0026ndash;config PG_USE_COPY YES\n Some more raster operations Hillshade and Terrain map library(raster) alt \u0026lt;- raster::getData(\u0026quot;alt\u0026quot;, country = \u0026quot;GBR\u0026quot;) slope \u0026lt;- raster::terrain(alt, opt = \u0026quot;slope\u0026quot;) aspect \u0026lt;- raster::terrain(alt, opt = \u0026quot;aspect\u0026quot;) hill \u0026lt;- raster::hillShade(slope, aspect, angle = 40, direction = 270) # plot your raster and newly extracted polygons on a map raster::plot(hill, col = grey(0:100/100), legend = FALSE) plot(sf::st_transform(new_extract, 4326), add = TRUE) raster::plot(alt, col = terrain.colors(25, alpha = 0.5), add = TRUE)  PART V: Plot your spatial object data with ggplot2 library(ggplot2) library(ggspatial) country_sf_gbr \u0026lt;- sf::st_as_sf(raster::getData(name = \u0026quot;GADM\u0026quot;, country = 'GBR', level = 1)) country_sf_gbr_osgb \u0026lt;- sf::st_transform(country_sf_gbr, crs = 27700) point_liverpool \u0026lt;- data.frame(name = 'Liverpool', longitude = -2.98, latitude = 53.41) point_edinburgh \u0026lt;- data.frame(name = 'Edinburgh', longitude = -3.19, latitude = 55.95) city_points \u0026lt;- rbind(point_liverpool, point_edinburgh) proj_city_points_osgb \u0026lt;- sf::st_transform(sf::st_as_sf(city_points, coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;), crs = 4326), crs = 27700) ## simple plot with ggplot ggplot(data = country_sf_gbr_osgb) + geom_sf() + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) ## some colors ggplot(data = country_sf_gbr_osgb) + geom_sf(color = \u0026quot;black\u0026quot;, fill = \u0026quot;wheat1\u0026quot; ) + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) ## color with some qualitative meaning ggplot(data = country_sf_gbr_osgb) + geom_sf(aes(fill = as.numeric(sf::st_area(country_sf_gbr_osgb)))) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;, name = \u0026quot;Area\u0026quot;) + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) # add a scale bar and a north arrow ggplot(data = country_sf_gbr_osgb) + geom_sf() + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + annotation_scale(location = \u0026quot;bl\u0026quot;, width_hint = 0.5) + annotation_north_arrow(location = \u0026quot;bl\u0026quot;, which_north = \u0026quot;true\u0026quot;, pad_x = unit(0.75, \u0026quot;in\u0026quot;), pad_y = unit(0.5, \u0026quot;in\u0026quot;), style = north_arrow_fancy_orienteering) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) # add more spatial object with geom_sf ggplot(data = country_sf_gbr_osgb) + geom_sf() + geom_sf(data = proj_city_points_osgb, size = 4, color = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;)) + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,1], y = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,2], label = country_sf_gbr_osgb$NAME_1, fontface = \u0026quot;italic\u0026quot;, color = \u0026quot;grey22\u0026quot;, size = 3) + annotation_scale(location = \u0026quot;bl\u0026quot;, width_hint = 0.5) + annotation_north_arrow(location = \u0026quot;bl\u0026quot;, which_north = \u0026quot;true\u0026quot;, pad_x = unit(0.75, \u0026quot;in\u0026quot;), pad_y = unit(0.5, \u0026quot;in\u0026quot;), style = north_arrow_fancy_orienteering) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;))  Adding a raster to a ggplot library(raster) alt \u0026lt;- raster::getData(\u0026quot;alt\u0026quot;, country = \u0026quot;GBR\u0026quot;) # need to reproject your raster to match the other layer and get a regular resolution alt_prj \u0026lt;- raster::projectRaster(alt, crs = sp::CRS(\u0026quot;+init=epsg:27700\u0026quot;), res = 1000) # first method alt_prj_df \u0026lt;- raster::as.data.frame(alt_prj, xy = TRUE) colnames(alt_prj_df) \u0026lt;- c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;Altitude\u0026quot;) ggplot() + geom_tile(data = alt_prj_df, aes(x = x, y = y, fill = Altitude)) + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,1], y = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,2], label = country_sf_gbr_osgb$NAME_1, fontface = \u0026quot;italic\u0026quot;, color = \u0026quot;white\u0026quot;, size = 3) + annotation_scale(location = \u0026quot;bl\u0026quot;, width_hint = 0.5) + annotation_north_arrow(location = \u0026quot;bl\u0026quot;, which_north = \u0026quot;true\u0026quot;, pad_x = unit(0.75, \u0026quot;in\u0026quot;), pad_y = unit(0.5, \u0026quot;in\u0026quot;), style = north_arrow_fancy_orienteering) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) + theme(panel.background = element_rect(fill = \u0026quot;aliceblue\u0026quot;)) ## second method (better) alt_prj_spdf \u0026lt;- as(alt_prj, \u0026quot;SpatialPixelsDataFrame\u0026quot;) alt_prj_df2 \u0026lt;- as.data.frame(alt_prj_spdf) colnames(alt_prj_df2) \u0026lt;- c(\u0026quot;Altitude\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;) ggplot() + geom_sf(data = country_sf_gbr_osgb) + geom_tile(data = alt_prj_df2, aes(x = x, y = y, fill = Altitude)) + geom_sf(data = proj_city_points_osgb, size = 4, color = c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;)) + xlab(\u0026quot;Longitude\u0026quot;) + ylab(\u0026quot;Latitude\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,1], y = sf::st_coordinates(sf::st_centroid(country_sf_gbr_osgb))[,2], label = country_sf_gbr_osgb$NAME_1, fontface = \u0026quot;italic\u0026quot;, color = \u0026quot;white\u0026quot;, size = 3) + annotation_scale(location = \u0026quot;bl\u0026quot;, width_hint = 0.5) + annotation_north_arrow(location = \u0026quot;bl\u0026quot;,which_north = \u0026quot;true\u0026quot;, pad_x = unit(0.75, \u0026quot;in\u0026quot;), pad_y = unit(0.5, \u0026quot;in\u0026quot;), style = north_arrow_fancy_orienteering) + ggtitle(\u0026quot;GBR map\u0026quot;, subtitle = paste0(\u0026quot;(\u0026quot;, length(unique(country_sf_gbr_osgb$NAME_1)), \u0026quot; countries)\u0026quot;)) + theme(panel.background = element_rect(fill = \u0026quot;aliceblue\u0026quot;))  Can you plot the deer occurrence extracted from GBIF on a UK map? library(rgbif) deer_locations \u0026lt;- occ_search(scientificName = \u0026quot;Cervus elaphus\u0026quot;, limit = 5000, hasCoordinate = TRUE, country = \u0026quot;GB\u0026quot;, return = \u0026quot;data\u0026quot;) %\u0026gt;% dplyr::select(key, name, decimalLongitude, decimalLatitude, year, individualCount, datasetKey, country) deer_locations_sf \u0026lt;- sf::st_as_sf(deer_locations, coords = c(\u0026quot;decimalLongitude\u0026quot;, \u0026quot;decimalLatitude\u0026quot;), crs = 4326) plot(deer_locations_sf[, \u0026quot;datasetKey\u0026quot;]) # ============ # YOUR TURN! # ===========  Your challenge, build some wider map for Europe Have a look at tutorial 1, 2 and 3 on how to draw beautiful map with sf.\n https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html https://www.r-spatial.org/r/2018/10/25/ggplot2-sf-2.html https://www.r-spatial.org/r/2018/10/25/ggplot2-sf-3.html  Look at the exellent sf vignettes (see list here: https://cran.r-project.org/web/packages/sf/index.html)\n","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"8f929e0eb027a441b0543396ad40433f","permalink":"https://BES-QSIG.github.io/courses/advancingr-spatial/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/courses/advancingr-spatial/","section":"courses","summary":"Part I: Preliminaries Required packages For this \u0026ldquo;tutorial\u0026rdquo;, you will need some packages and their dependencies\nif(!requireNamespace(\u0026quot;raster\u0026quot;)) install.packages(\u0026quot;raster\u0026quot;) if(!requireNamespace(\u0026quot;sf\u0026quot;)) install.packages(\u0026quot;sf\u0026quot;) if(!requireNamespace(\u0026quot;ggplot2\u0026quot;)) install.packages(\u0026quot;ggplot2\u0026quot;) if(!requireNamespace(\u0026quot;ggspatial\u0026quot;)) install.packages(\u0026quot;ggspatial\u0026quot;) if(!requireNamespace(\u0026quot;rgdal\u0026quot;)) install.packages(\u0026quot;rgdal\u0026quot;) if(!requireNamespace(\u0026quot;RPostgreSQL\u0026quot;)) install.packages(\u0026quot;RPostgreSQL\u0026quot;) if(!requireNamespace(\u0026quot;rgbif\u0026quot;)) install.packages(\u0026quot;rgbif\u0026quot;) library(raster) library(sf) library(ggplot2) library(ggspatial) library(RPostgreSQL) library(rgdal) library(rgbif)  The data for this tutorial are available from here.\nWhat is a spatial object? A spatial object is an entity with coordinates in a geographical space (x, y) or (x, y, z) with a specific projection.","tags":null,"title":"Day Two: Spatial data manipulation and visualisation in R","type":"docs"},{"authors":null,"categories":null,"content":" The British Ecological Society Quantitative Ecology Group are hosting a two-day R workshop which will cover reproducible data manipulation and visualisation workflows in R. Day One will be focussed on the tidyverse, while Day Two will move into working with spatial data.\nDay One: Data manipulation and visualisation in R using the tidyverse Workshop leader: Gergana Daskalova, University of Edinburgh, Coding Club The tidyverse collection of packages (tidyr, dplyr, ggplot2, broom and more) is a great tool for efficient data manipulation, visualisation and analysis. In this workshop, you’ll learn how to manipulate data using pipes, build functions and apply them in a pipe, summarise model output and visualise data analysis results. We’ll then delve into functional programming using purr. Along the way, we’ll be using large ecological datasets, so you will get hands-on experience with how you can tackle big data in ecology and achieve a lot in not that many lines of code. You’ll gain skills in customising different types of graphs to maximise the impact of your data visualisation and finally, we will be doing all of this under the Markdown framework for reproducible report to promote transparency in research.\nDay Two: Spatial data manipulation and visualisation in R Workshop leader: Reto Schmucki, Centre for Ecology and Hydrology Geocomputing and manipulating geographic data are important skills for modelling spatial phenomena and have become increasingly important in data science. Over the last decade, there has been important progress in open-source GIS. In this workshop, you\u0026rsquo;ll learn how to create, import and manipulate spatial data, using the rich spatial ecosystem available in R (sf, sp, raster, tmap, leaflet). With emphasis on Simple Features for R available through the sf package. You will learn how to extract spatial information, merge complex data sets, visualise geographic data and produce interactive maps. Along the way, we\u0026rsquo;ll work on real-word data and explore how to harness the power of R and how it links to other open-source GIS solutions (QGIS, GRASS, PostGIS). At the end of this workshop, you will be able to implement a workflow to manipulate data, run analysis and visualise your results, using open-source solutions with a transparent and fully reproducible workflow.\nThe option is available to come to both days, or just one of the two. Lunch and refreshments will be provided.\nSOLD OUT\n","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"01f95e0909220296e198cc89ee8d497d","permalink":"https://BES-QSIG.github.io/events/2018-12-3-advancing-your-r/","publishdate":"2018-11-13T00:00:00Z","relpermalink":"/events/2018-12-3-advancing-your-r/","section":"events","summary":"Want to build on your R skills for more transparant and reproducible data analysis workflows? This is the course for you!","tags":["R","spatial","tidyverse","Courses"],"title":"Advancing your R","type":"events"},{"authors":null,"categories":null,"content":"    Laura Graham (Secretary)\nUniversity of Southampton    Twitter |   ResearchGate |   Website      Laura is a spatial/computational ecologist who is interested in what impacts environmental changes have on species’ distributions and how we can use this information in landscape and conservation planning to reduce detrimental effects. In a previous career, Laura was a mathematician and database developer, and applies skills learned there to answer ecological questions.\n    Gergana Daskalova (Training and Education Rep)\nUniversity of Edinburgh    Twitter |   Scholar |   Website |   Email      Gergana is a global change ecologist with a passion for biodiversity change, agro-ecology and quantitative syntheses. The overarching aim of her research is to determine the sources of the complex patterns of population and biodiversity change observed over time around the world. From forest cover change around the world to climate warming in the Arctic and more, she is investigating how and why Earth’s biota is changing across the Anthropocene.\n    Dominic Bennett (Online Resources Coordinator)\nUniversity of Gothenburg    Twitter |   Scholar      Dom did a PhD in macroevolution at Imperial College London and the Zoological Society of London on the legitimacy of the term “living fossil”. He now works as an open-source developer in phylogenetics and biodiversity informatics as part of a postdoctoral position at the University of Gothenburg, Sweden.\nRick Stafford (Ordinary Member)\nBournemouth University\nRick is a Principal Academic in Computational Ecology at Bournemouth University. His research involves the application of statistical and computational techniques to a wide variety of systems, particularly the structure of marine communities. Rick is interested in identification of novel biomarkers, and how these can be linked to changes in community structure; and also in the development of models with simple and intuitive interfaces, which can be used in topics as diverse as policy, management or education.\n Twitter\n    Liam Butler (Training and Education Rep)\nNewcastle University    Twitter |   ResearchGate      Liam is a biological modeller with special focus on vegetation distribution modelling. His research focuses on understanding how vegetation patterns change at multiple scales, using data ranging from 10 cm scale to field and landscape scale. The main aim is to understand patterns in individual plants, plant communities, and catchment landscapes, relating them to small-scale environmental conditions, field-scale grazing through to broad-scale catchment management. This will allow plant distributions to be predicted at fine spatial scales from relatively coarse resolution remote-sensed data.\n    Susan Jarvis (Ordinary Member)\nCentre for Ecology \u0026 Hydrology    Twitter |   Scholar      Susan is a quantitative ecologist at the Centre for Ecology and Hydrology in Lancaster. She is interested in understanding the processes behind species distributions, diversity and community composition and uses a range of methods including species distribution models and multivariate statistics. She is particularly interested in methods to combine multiple datasets (e.g. professional surveys and citizen science) to answer ecological questions. Susan’s background is in fungal ecology but she currently works with a broad range of study organisms from plants to bees.\n    Phil Bouchet (Website Coordinator)\nUniversity of St Andrews    Twitter |   Scholar |   ResearchGate |   Website      Phil is a quantitative marine ecologist with an active interest in the use of GIS and statistical methods to address pressing wildlife conservation problems in the ocean. Phil’s background is in marine mammal science, although his work encompasses a range of other species including sharks, pelagic fishes, turtles and seabirds. Phil’s interests lie in the estimation of animal abundance and distribution, using frequentist and Bayesian methods. He is currently a Research Associate at the Centre for Research into Ecological \u0026amp; Environmental Modelling (CREEM) at the University of St Andrews, where he focuses on extrapolation in cetacean density models.\nTom August (Social Media Rep)\nCentre for Ecology \u0026amp; Hydrology\nTom is a computational ecologist at the Centre for Ecology \u0026amp; Hydrology. Tom’s work focuses on the nexus between citizen science, technology, and data science. Tom works with statistical experts to develop methods for analysing species occurrence data and works to make these methods available to other academics and practitioners. Tom’s work spans a range of tools and technologies including; AI, computer vision, drones, mobile phone applications and web platforms.\n Twitter\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e079bfc3341cf93ccbfcec89544f6c6c","permalink":"https://BES-QSIG.github.io/about/committee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/committee/","section":"about","summary":"Laura Graham (Secretary)\nUniversity of Southampton    Twitter |   ResearchGate |   Website      Laura is a spatial/computational ecologist who is interested in what impacts environmental changes have on species’ distributions and how we can use this information in landscape and conservation planning to reduce detrimental effects. In a previous career, Laura was a mathematician and database developer, and applies skills learned there to answer ecological questions.","tags":null,"title":"Commitee Members","type":"about"}]